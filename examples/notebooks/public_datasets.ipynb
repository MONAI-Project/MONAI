{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start With Public Datasets and add new Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we introduce how to quickly set up workflows with MONAI public Datasets and how to add new Dataset.  \n",
    "Currently, MONAI provides `MedNISTDataset` and `DecathlonDataset` to automatically download,  \n",
    "extract MedNIST dataset and Decathlon dataset, and act as PyTorch datasets to generate training/validation/test data. \n",
    "\n",
    "We'll cover the following topics in this tutorial:\n",
    "<ul>\n",
    "    <li>Create training experiment with MedNISTDataset and workflow</li>\n",
    "    <li>Create training experiment with DecathlonDataset and workflow</li>\n",
    "    <li>Share other public data and add Dataset in MONAI</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import tempfile\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Callable, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from ignite.metrics import Accuracy\n",
    "from monai.data import DataLoader, CacheDataset\n",
    "from monai.apps import MedNISTDataset, DecathlonDataset, download_and_extract\n",
    "from monai.transforms import \\\n",
    "    LoadPNGd, AddChanneld, ScaleIntensityd, ToTensord, Compose, Randomizable, \\\n",
    "    AsDiscreted, LoadNiftid, Spacingd, Orientationd, Resized, AsDiscreted\n",
    "from monai.networks.nets import densenet121, UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.losses import DiceLoss\n",
    "from monai.engines import SupervisedTrainer\n",
    "from monai.inferers import SimpleInferer\n",
    "from monai.handlers import StatsHandler, MeanDice\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training experiment with MedNISTDataset and workflow\n",
    "The MedNIST dataset was gathered from several sets from [TCIA](https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions), [the RSNA Bone Age Challenge](http://rsnachallenges.cloudapp.net/competitions/4), and [the NIH Chest X-ray dataset](https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pre-processing transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        LoadPNGd(keys='image'),\n",
    "        AddChanneld(keys='image'),\n",
    "        ScaleIntensityd(keys='image'),\n",
    "        ToTensord(keys='image')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MedNISTDataset for training\n",
    "`MedNISTDataset` inherits from MONAI `CacheDataset` and provides rich parameters to achieve expected behavior:\n",
    "1. **root_dir**: target directory to download and load MedNIST dataset.\n",
    "2. **section**: expected data section, can be: `training`, `validation` or `test`.\n",
    "3. **transform**: transforms to execute operations on input data. the default transform is composed by `LoadPNGd` and `AddChanneld`, which can load data into numpy array with [C, H, W] shape.\n",
    "4. **download**: whether to download and extract the MedNIST from resource link, default is False. if expected file already exists, skip downloading even set it to True. user can manually copy `MedNIST.tar.gz` file or `MedNIST` folder to root directory.\n",
    "5. **seed**: random seed to randomly split training, validation and test datasets, defaut is 0.\n",
    "6. **val_frac**: percentage of of validation fraction in the whole dataset, default is 0.1.\n",
    "7. **test_frac**: percentage of of test fraction in the whole dataset, default is 0.1.\n",
    "8. **cache_num**: number of items to be cached. Default is `sys.maxsize`. will take the minimum of (cache_num, data_length x cache_rate, data_length).\n",
    "9. **cache_rate**: percentage of cached data in total, default is 1.0 (cache all). will take the minimum of (cache_num, data_length x cache_rate, data_length).\n",
    "10. **num_workers**: the number of worker threads to use. If 0 a single thread will be used. Default is 0.\n",
    "\n",
    "Note that the \"tar files\" are cached after a first time downloading. the `self.__getitem__()` API generates 1 `{\"image\": XXX, \"label\": XXX}` dict according to the specified index within the `self.__len__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = tempfile.mkdtemp()\n",
    "train_ds = MedNISTDataset(root_dir=tempdir, transform=transform, section=\"training\", download=True)\n",
    "# the dataset can work seamlessly with the pytorch native dataset loader,\n",
    "# but using monai.data.DataLoader has additional benefits of mutli-process\n",
    "# random seeds handling, and the customized collate functions\n",
    "train_loader = DataLoader(train_ds, batch_size=300, shuffle=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick images from MedNISTDataset to visualize and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(train_ds[i * 5000]['image'][0].detach().cpu(), cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "net = densenet121(spatial_dims=2, in_channels=1, out_channels=6).to(device)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(net.parameters(), 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the easiest training workflow and run\n",
    "Use MONAI SupervisedTrainer handlers to quickly set up a training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SupervisedTrainer(\n",
    "    device=device,\n",
    "    max_epochs=5,\n",
    "    train_data_loader=train_loader,\n",
    "    network=net,\n",
    "    optimizer=opt,\n",
    "    loss_function=loss,\n",
    "    inferer=SimpleInferer(),\n",
    "    key_train_metric={'train_acc': Accuracy(output_transform=lambda x: (x['pred'], x['label']))},\n",
    "    train_handlers=StatsHandler(tag_name='train_loss', output_transform=lambda x: x['loss']),\n",
    ")\n",
    "trainer.run()\n",
    "\n",
    "shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training experiment with DecathlonDataset and workflow\n",
    "The Decathlon dataset came from [Medical Segmentation Decathlon](http://medicaldecathlon.com/) AI challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pre-processing transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        LoadNiftid(keys=['image', 'label']),\n",
    "        AddChanneld(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1., 1., 1.), mode=('bilinear', 'nearest')),\n",
    "        Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "        ScaleIntensityd(keys='image'),\n",
    "        Resized(keys=['image', 'label'], spatial_size=(32, 64, 32), mode=('trilinear', 'nearest')),\n",
    "        ToTensord(keys=['image', 'label']),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DeccathlonDataset for training\n",
    "`DecathlonDataset` inherits from MONAI `CacheDataset` and provides rich parameters to achieve expected behavior:\n",
    "1. **root_dir**: user's local directory for caching and loading the MSD datasets.\n",
    "2. **task**: which task to download and execute: one of list (\"Task01_BrainTumour\", \"Task02_Heart\", \"Task03_Liver\", \"Task04_Hippocampus\", \"Task05_Prostate\", \"Task06_Lung\", \"Task07_Pancreas\", \"Task08_HepaticVessel\", \"Task09_Spleen\", \"Task10_Colon\").\n",
    "3. **section**: expected data section, can be: `training`, `validation` or `test`.\n",
    "4. **transform**: transforms to execute operations on input data. the default transform is composed by `LoadNiftid` and `AddChanneld`, which can load data into numpy array with [C, H, W, D] shape.\n",
    "5. **download**: whether to download and extract the Decathlon from resource link, default is False. if expected file already exists, skip downloading even set it to True. user can manually copy tar file or dataset folder to the root directory.\n",
    "6. **seed**: random seed to randomly split `training`, `validation` and `test` datasets, defaut is 0.\n",
    "7. **val_frac**: percentage of of validation fraction from the `training` section, default is 0.2. Decathlon data only contains `training` section with labels and `test` section without labels, so randomly select fraction from the `training` section as the `validation` section.\n",
    "8. **cache_num**: number of items to be cached. Default is `sys.maxsize`. will take the minimum of (cache_num, data_length x cache_rate, data_length).\n",
    "9. **cache_rate**: percentage of cached data in total, default is 1.0 (cache all). will take the minimum of (cache_num, data_length x cache_rate, data_length).\n",
    "10. **num_workers**: the number of worker threads to use. if 0 a single thread will be used. Default is 0.\n",
    "\n",
    "Note that the \"tar files\" are cached after a first time downloading. the `self.__getitem__()` API generates 1 `{\"image\": XXX, \"label\": XXX}` dict according to the specified index within the `self.__len__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = tempfile.mkdtemp()\n",
    "train_ds = DecathlonDataset(root_dir=tempdir, task='Task04_Hippocampus', transform=transform,\n",
    "                            section='training', download=True)\n",
    "# the dataset can work seamlessly with the pytorch native dataset loader,\n",
    "# but using monai.data.DataLoader has additional benefits of mutli-process\n",
    "# random seeds handling, and the customized collate functions\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick images from DecathlonDataset to visualize and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(3, 3, figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(train_ds[i * 20]['image'][0, :, :, 10].detach().cpu(), cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "net = UNet(dimensions=3, in_channels=1, out_channels=3, channels=(16, 32, 64, 128, 256),\n",
    "           strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH).to(device)\n",
    "loss = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "opt = torch.optim.Adam(net.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the easiest training workflow and run\n",
    "Use MONAI SupervisedTrainer handlers to quickly set up a training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SupervisedTrainer(\n",
    "    device=device,\n",
    "    max_epochs=5,\n",
    "    train_data_loader=train_loader,\n",
    "    network=net,\n",
    "    optimizer=opt,\n",
    "    loss_function=loss,\n",
    "    inferer=SimpleInferer(),\n",
    "    post_transform=AsDiscreted(keys=['pred', 'label'], argmax=(True, False), to_onehot=True, n_classes=3),\n",
    "    key_train_metric={'train_meandice': MeanDice(output_transform=lambda x: (x['pred'], x['label']))},\n",
    "    train_handlers=StatsHandler(tag_name='train_loss', output_transform=lambda x: x['loss']),\n",
    ")\n",
    "trainer.run()\n",
    "\n",
    "shutil.rmtree(tempdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share other public data and add Dataset in MONAI\n",
    "Referring to the `MedNISTDataset` or `DecathlonDataset`, it's easy to create a new Dataset fo other public data.  \n",
    "Mainly include below steps:\n",
    "1. Inherit MONAI `CacheDataset` to leverage caching mechanism to accelerate training.\n",
    "1. Make sure the lisence of dataset allows public access and share.\n",
    "2. Use `monai.apps.download_and_extract` to download and extract data in the Dataset.\n",
    "3. Define the logic to randomly split `training`, `validation` and `test` sections.\n",
    "4. Construct data list with `dict` items:\n",
    "```py\n",
    "[\n",
    "    {'image': image1_path, 'label': label1_path},\n",
    "    {'image': image2_path, 'label': label2_path},\n",
    "    {'image': image3_path, 'label': label3_path},\n",
    "    ... ...\n",
    "]\n",
    "```\n",
    "5. Define dataset specific logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define IXIDataset as an example\n",
    "Here we use the [IXI Dataset](https://brain-development.org/ixi-dataset/) as an example to show how to create a new `IXIDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IXIDataset(Randomizable, CacheDataset):\n",
    "    resource = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar'\n",
    "    md5 = '34901a0593b41dd19c1a1f746eac2d58'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        section: str,\n",
    "        transform: Callable[..., Any],\n",
    "        download: bool = False,\n",
    "        seed: int = 0,\n",
    "        val_frac: float = 0.2,\n",
    "        test_frac: float = 0.2,\n",
    "        cache_num: int = sys.maxsize,\n",
    "        cache_rate: float = 1.0,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError('root_dir must be a directory.')\n",
    "        self.section = section\n",
    "        self.val_frac = val_frac\n",
    "        self.test_frac = test_frac\n",
    "        self.set_random_state(seed=seed)\n",
    "        dataset_dir = os.path.join(root_dir, 'ixi')\n",
    "        tarfile_name = f\"{dataset_dir}.tar\"\n",
    "        if download:\n",
    "            download_and_extract(self.resource, tarfile_name, dataset_dir, self.md5)\n",
    "        # as a quick demo, we just use 10 images to show\n",
    "        self.datalist = [\n",
    "            {'image': os.path.join(dataset_dir, 'IXI314-IOP-0889-T1.nii.gz'), 'label': 0},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI249-Guys-1072-T1.nii.gz'), 'label': 0},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI609-HH-2600-T1.nii.gz'), 'label': 0},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI173-HH-1590-T1.nii.gz'), 'label': 1},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI020-Guys-0700-T1.nii.gz'), 'label': 0},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI342-Guys-0909-T1.nii.gz'), 'label': 0},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI134-Guys-0780-T1.nii.gz'), 'label': 0},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI577-HH-2661-T1.nii.gz'), 'label': 1},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI066-Guys-0731-T1.nii.gz'), 'label': 1},\n",
    "            {'image': os.path.join(dataset_dir, 'IXI130-HH-1528-T1.nii.gz'), 'label': 0}\n",
    "        ]\n",
    "        data = self._generate_data_list()\n",
    "        super().__init__(data, transform, cache_num=cache_num, cache_rate=cache_rate, num_workers=num_workers)\n",
    "\n",
    "\n",
    "    def randomize(self, data: Optional[Any] = None):\n",
    "        self.rann = self.R.random()\n",
    "\n",
    "    def _generate_data_list(self):\n",
    "        data = list()\n",
    "        for d in self.datalist:\n",
    "            self.randomize()\n",
    "            if self.section == 'training':\n",
    "                if self.rann < self.val_frac + self.test_frac:\n",
    "                    continue\n",
    "            elif self.section == 'validation':\n",
    "                if self.rann >= self.val_frac:\n",
    "                    continue\n",
    "            elif self.section == 'test':\n",
    "                if self.rann < self.val_frac or self.rann >= self.val_frac + self.test_frac:\n",
    "                    continue\n",
    "            else:\n",
    "                raise ValueError('section name can only be: training, validation or test.')\n",
    "            data.append(d)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick images from IXIDataset to visualize and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdir = tempfile.mkdtemp()\n",
    "train_ds = IXIDataset(\n",
    "    root_dir=tempdir,\n",
    "    section='training',\n",
    "    transform=Compose([LoadNiftid('image'), ToTensord('image')]),\n",
    "    download=True\n",
    ")\n",
    "plt.figure('check', (18, 6))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(train_ds[i]['image'][:, :, 80].detach().cpu(), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "shutil.rmtree(tempdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}