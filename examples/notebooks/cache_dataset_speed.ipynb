{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Dataset Tutorial and Speed Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to accelerate PyTorch medical DL program based on MONAI CacheDataset.  \n",
    "It's modified from the Spleen 3D segmentation tutorial notebook.\n",
    "\n",
    "The Spleen dataset can be downloaded from http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monai.data import Dataset, CacheDataset, list_data_collate\n",
    "from monai.transforms import \\\n",
    "    Compose, LoadNiftid, AddChanneld, ScaleIntensityRanged, CropForegroundd, \\\n",
    "    RandCropByPosNegLabeld, RandAffined, Spacingd, Orientationd, ToTensord\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.layers import Norm\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import compute_meandice\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set MSD Spleen dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/workspace/data/medical/Task09_Spleen'\n",
    "train_images = sorted(glob.glob(os.path.join(data_root, 'imagesTr', '*.nii.gz')))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_root, 'labelsTr', '*.nii.gz')))\n",
    "data_dicts = [{'image': image_name, 'label': label_name}\n",
    "              for image_name, label_name in zip(train_images, train_labels)]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation\n",
    "  \n",
    "Deterministic transforms during training:  \n",
    "- LoadNiftid  \n",
    "- AddChanneld  \n",
    "- Spacingd  \n",
    "- Orientationd  \n",
    "- ScaleIntensityRanged  \n",
    "  \n",
    "Non-deterministic transforms:  \n",
    "- RandCropByPosNegLabeld  \n",
    "- ToTensord\n",
    "\n",
    "All the validation transforms are deterministic.  \n",
    "The results of all the deterministic transforms will be cached to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations():\n",
    "    train_transforms = Compose([\n",
    "        LoadNiftid(keys=['image', 'label']),\n",
    "        AddChanneld(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), mode=('bilinear', 'nearest')),\n",
    "        Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "        ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        # randomly crop out patch samples from big image based on pos / neg ratio\n",
    "        # the image centers of negative samples must be in valid image area\n",
    "        RandCropByPosNegLabeld(keys=['image', 'label'], label_key='label', spatial_size=(96, 96, 96), pos=1,\n",
    "                               neg=1, num_samples=4, image_key='image', image_threshold=0),\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ])\n",
    "    val_transforms = Compose([\n",
    "        LoadNiftid(keys=['image', 'label']),\n",
    "        AddChanneld(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), mode=('bilinear', 'nearest')),\n",
    "        Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "        ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ])\n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_process(train_ds, val_ds):\n",
    "    \n",
    "    # use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "    # to generate 2 x 4 images for network training\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=list_data_collate)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)\n",
    "    device = torch.device('cuda:0')\n",
    "    model = UNet(dimensions=3, in_channels=1, out_channels=2, channels=(16, 32, 64, 128, 256),\n",
    "                 strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH).to(device)\n",
    "    loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "    epoch_num = 600\n",
    "    val_interval = 1  # do validation for every epoch\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "    epoch_times = list()\n",
    "    total_start = time.time()\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start = time.time()\n",
    "        print('-' * 10)\n",
    "        print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step_start = time.time()\n",
    "            step += 1\n",
    "            inputs, labels = batch_data['image'].to(device), batch_data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(f\"{step}/{len(train_ds) // train_loader.batch_size}, train_loss: {loss.item():.4f}\"\n",
    "                  f\" step time: {(time.time() - step_start):.4f}\")\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                metric_sum = 0.\n",
    "                metric_count = 0\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = val_data['image'].to(device), val_data['label'].to(device)\n",
    "                    roi_size = (160, 160, 160)\n",
    "                    sw_batch_size = 4\n",
    "                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                    value = compute_meandice(y_pred=val_outputs, y=val_labels, include_background=False,\n",
    "                                             to_onehot_y=True, mutually_exclusive=True)\n",
    "                    metric_count += len(value)\n",
    "                    metric_sum += value.sum().item()\n",
    "                metric = metric_sum / metric_count\n",
    "                metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), 'best_metric_model.pth')\n",
    "                    print('saved new best metric model')\n",
    "                print(f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                      f\" best mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "        print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "        epoch_times.append(time.time() - epoch_start)\n",
    "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "          f\" total time: {(time.time() - total_start):.4f}\")\n",
    "    return epoch_num, time.time() - total_start, epoch_loss_values, metric_values, epoch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and define regular Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "train_trans, val_trans = transformations()\n",
    "train_ds = Dataset(data=train_files, transform=train_trans)\n",
    "val_ds = Dataset(data=val_files, transform=val_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with regular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num, total_time, epoch_loss_values, metric_values, epoch_times = train_process(train_ds, val_ds)\n",
    "print(f\"total training time of {epoch_num} epochs with regular Dataset: {total_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and define Cache Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "train_trans, val_trans = transformations()\n",
    "cache_init_start = time.time()\n",
    "cache_train_ds = CacheDataset(data=train_files, transform=train_trans, cache_rate=1.0, num_workers=4)\n",
    "cache_val_ds = CacheDataset(data=val_files, transform=val_trans, cache_rate=1.0, num_workers=4)\n",
    "cache_init_time = time.time() - cache_init_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Cache Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num, cache_total_time, cache_epoch_loss_values, cache_metric_values, cache_epoch_times = \\\n",
    "    train_process(cache_train_ds, cache_val_ds)\n",
    "print(f\"total training time of {epoch_num} epochs with CacheDataset: {cache_total_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('train', (12, 12))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Regular Epoch Average Loss')\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='red')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Regular Val Mean Dice')\n",
    "x = [i + 1 for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='red')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Cache Epoch Average Loss')\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='green')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Cache Val Mean Dice')\n",
    "x = [i + 1 for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total time and every epoch time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('train', (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Total Train Time(600 epochs)')\n",
    "plt.bar('regular', total_time, 1, label='Regular Dataset', color='red')\n",
    "plt.bar('cache', cache_init_time + cache_total_time, 1, label='Cache Dataset', color='green')\n",
    "plt.bar('cache', cache_init_time, 1, label='Cache Init', color='orange')\n",
    "plt.ylabel('secs')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Epoch Time')\n",
    "x = [i + 1 for i in range(len(epoch_times))]\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('secs')\n",
    "plt.plot(x, epoch_times, label='Regular Dataset', color='red')\n",
    "plt.plot(x, cache_epoch_times, label='Cache Dataset', color='green')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}