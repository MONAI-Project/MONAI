{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LL5qtxXr-uF3"
   },
   "source": [
    "[Google colab version](https://colab.research.google.com/drive/15wJus5WZPYxTYE51yBhIBNhk9Tj4k3BT?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4bkNVc42_H9r"
   },
   "source": [
    "# 3D segmentation with [MONAI](https://github.com/Project-MONAI/MONAI) and [Catalyst](https://github.com/catalyst-team/catalyst)\n",
    "\n",
    "This tutorial demonstrates how [MONAI](https://github.com/Project-MONAI/MONAI) can be used with the [Catalyst](https://github.com/catalyst-team/catalyst) framework for 3D segmentation task.\n",
    "And easily use below features:\n",
    "\n",
    "- Prepare synthetic data.\n",
    "- Load Nifti image with metadata.\n",
    "- Transforms for dictionary format data.\n",
    "- Add channel dim to the data if no channel dimension.\n",
    "- Scale medical image intensity with expected range.\n",
    "- Crop out a batch of balanced images based on positive / negative label ratio.\n",
    "- 3D UNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
    "- Sliding window inference method.\n",
    "- Deterministic training for reproducibility.\n",
    "\n",
    "This tutorial is based on [unet_training_dict.py](https://github.com/Project-MONAI/MONAI/blob/master/examples/segmentation_3d/unet_training_dict.py) and [spleen_segmentation_3d.ipynb](https://github.com/Project-MONAI/MONAI/blob/master/examples/notebooks/spleen_segmentation_3d.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGYGdy1yBLr5"
   },
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "el520dTNfmZm",
    "outputId": "6668fb1a-3c73-4cc3-97b6-8d8260b33433"
   },
   "outputs": [],
   "source": [
    "! pip install -q catalyst==20.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "r1P0mVvymvsY",
    "outputId": "930a36c6-bbb7-49a6-9520-8335df0b9164"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "from glob import glob\n",
    "import logging\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadNiftid,\n",
    "    AsChannelFirstd,\n",
    "    ScaleIntensityd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandRotate90d,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.data import create_test_image_3d, list_data_collate\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "from catalyst import dl\n",
    "\n",
    "monai.config.print_config()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJpDdAo1-ZF4"
   },
   "source": [
    "# [MONAI](https://github.com/Project-MONAI/MONAI) components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2Wo7P7EBRdQ"
   },
   "source": [
    "## Prepare synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eq3RgOT2BclX",
    "outputId": "3a5039ac-bb44-443a-bdc3-38fca80cc6eb"
   },
   "outputs": [],
   "source": [
    "# create a temporary directory and 40 random image, mask paris\n",
    "tempdir = tempfile.mkdtemp()\n",
    "print(f\"generating synthetic data to {tempdir} (this may take a while)\")\n",
    "for i in range(40):\n",
    "    im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n",
    "\n",
    "    n = nib.Nifti1Image(im, np.eye(4))\n",
    "    nib.save(n, os.path.join(tempdir, f\"img{i:d}.nii.gz\"))\n",
    "\n",
    "    n = nib.Nifti1Image(seg, np.eye(4))\n",
    "    nib.save(n, os.path.join(tempdir, f\"seg{i:d}.nii.gz\"))\n",
    "\n",
    "images = sorted(glob(os.path.join(tempdir, \"img*.nii.gz\")))\n",
    "segs = sorted(glob(os.path.join(tempdir, \"seg*.nii.gz\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WX6w-86XBjeh"
   },
   "source": [
    "## Prepare transforms and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CjeN90W-feaM",
    "outputId": "2493d49b-8457-4139-ae7a-3c5fa5bd9087"
   },
   "outputs": [],
   "source": [
    "train_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(images[:20], segs[:20])]\n",
    "val_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(images[-20:], segs[-20:])]\n",
    "\n",
    "# define transforms for image and segmentation\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadNiftid(keys=[\"img\", \"seg\"]),\n",
    "        AsChannelFirstd(keys=[\"img\", \"seg\"], channel_dim=-1),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"img\", \"seg\"], label_key=\"seg\", spatial_size=[96, 96, 96], pos=1, neg=1, num_samples=4\n",
    "        ),\n",
    "        RandRotate90d(keys=[\"img\", \"seg\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "        ToTensord(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadNiftid(keys=[\"img\", \"seg\"]),\n",
    "        AsChannelFirstd(keys=[\"img\", \"seg\"], channel_dim=-1),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "        ToTensord(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define dataset, data loader\n",
    "check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n",
    "check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, collate_fn=list_data_collate)\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(check_data[\"img\"].shape, check_data[\"seg\"].shape)\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=list_data_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "# create a validation data loader\n",
    "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4, collate_fn=list_data_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BB8EGm5OBuIR"
   },
   "source": [
    "## Prepare model, optimizer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIh_W821Bvdd"
   },
   "outputs": [],
   "source": [
    "# create UNet, DiceLoss and Adam optimizer\n",
    "# device = torch.device(\"cuda:0\")  # you don't need device, because Catalyst uses autoscaling\n",
    "model = monai.networks.nets.UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ")\n",
    "loss_function = monai.losses.DiceLoss(sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, to_onehot_y=False, sigmoid=True, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BSHNgJ2e8908"
   },
   "source": [
    "# [Catalyst](https://github.com/catalyst-team/catalyst) experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oO8ijD62CCvm"
   },
   "source": [
    "## Setup Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtiY6gC1CCb6"
   },
   "outputs": [],
   "source": [
    "class MonaiSupervisedRunner(dl.SupervisedRunner):\n",
    "\n",
    "  def forward(self, batch):\n",
    "    if self.is_train_loader:\n",
    "      output = {self.output_key: self.model(batch[self.input_key])}\n",
    "    elif self.is_valid_loader:\n",
    "      roi_size = (96, 96, 96)\n",
    "      sw_batch_size = 4\n",
    "      output = {self.output_key: \n",
    "                sliding_window_inference(batch[self.input_key], roi_size, sw_batch_size, self.model)}\n",
    "    elif self.is_infer_loader:\n",
    "      roi_size = (96, 96, 96)\n",
    "      sw_batch_size = 4\n",
    "      batch = self._batch2device(batch, self.device)\n",
    "      output = {self.output_key: \n",
    "                sliding_window_inference(batch[self.input_key], roi_size, sw_batch_size, self.model)}\n",
    "      output = {**output, **batch}\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVBQ44E3CJst"
   },
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "id": "NogjkLJaf-1U",
    "outputId": "d96f2f00-42f1-4863-ccfb-1a1facbbe652"
   },
   "outputs": [],
   "source": [
    "runner = MonaiSupervisedRunner(\n",
    "    input_key=\"img\", input_target_key=\"seg\", output_key=\"logits\")  # you can also specify `device` here\n",
    "runner.train(\n",
    "    loaders={\"train\": train_loader, \"valid\": val_loader},\n",
    "    model=model,\n",
    "    criterion=loss_function,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=6, logdir=\"./logs\", \n",
    "    main_metric=\"dice_metric\", minimize_metric=False,\n",
    "    verbose=False, timeit=True,  # let's use minimal logs, but with time checkers\n",
    "    callbacks={\n",
    "        \"loss\": dl.CriterionCallback(input_key=\"seg\", output_key=\"logits\"),\n",
    "        \"periodic_valid\": dl.PeriodicLoaderCallback(valid=2),\n",
    "        \"dice_metric\": dl.MetricCallback(prefix=\"dice_metric\", metric_fn=dice_metric, \n",
    "                                         input_key=\"seg\", output_key=\"logits\")\n",
    "    },\n",
    "    load_best_on_end=True,  # user-friendly API :)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugpGCkyS83e0"
   },
   "source": [
    "# Tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1RDmcUa8tQy"
   },
   "source": [
    "# Best model performance visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nHYP-ltLqzoC",
    "outputId": "5a75dac9-8f25-459f-cddc-cc8b9dfc4767"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, valid_output in enumerate(runner.predict_loader(loader=val_loader)):\n",
    "    if i > 4:\n",
    "        break\n",
    "    plt.figure('check', (9, 3))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('image ' + str(i))\n",
    "    plt.imshow(valid_output['img'].detach().cpu()[0, 0, :, :, 48], cmap='gray')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title('label ' + str(i))\n",
    "    plt.imshow(valid_output['seg'].detach().cpu()[0, 0, :, :, 48])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title('output ' + str(i))\n",
    "    logits = valid_output[\"logits\"]\n",
    "    plt.imshow((logits[0] > 0.5).float().detach().cpu()[0, :, :, 48])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iQXpWNwlUjG"
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(tempdir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2007.monai-catalyst.segmentation_3d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}