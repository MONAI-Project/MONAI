{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks with MedNIST Dataset\n",
    "\n",
    "## Introduction \n",
    "\n",
    "This notebook illustrates the use of MONAI for training a network to generate images from a random input tensor. A simple GAN is employed to do with a separate Generator and Discriminator networks. \n",
    "\n",
    "This will go through the steps of:\n",
    "* Loading the data from a remote source\n",
    "* Constructing a dataset from this data and transforms\n",
    "* Defining the networks\n",
    "* Training and evaluation\n",
    "\n",
    "### Get the dataset\n",
    "\n",
    "The MedNIST dataset was gathered from several sets from [TCIA](https://wiki.cancerimagingarchive.net/display/Public/Data+Usage+Policies+and+Restrictions), [the RSNA Bone Age Challenge](http://rsnachallenges.cloudapp.net/competitions/4), and [the NIH Chest X-ray dataset](https://cloud.google.com/healthcare/docs/resources/public-datasets/nih-chest).\n",
    "\n",
    "The dataset is kindly made available by [Dr. Bradley J. Erickson M.D., Ph.D.](https://www.mayo.edu/research/labs/radiology-informatics/overview) (Department of Radiology, Mayo Clinic)\n",
    "under the Creative Commons [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/).\n",
    "If you use the MedNIST dataset, please acknowledge the source, e.g.\n",
    "\n",
    "https://github.com/Project-MONAI/MONAI/blob/master/examples/notebooks/mednist_tutorial.ipynb.\n",
    "\n",
    "First step is to import libraries and define some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import monai\n",
    "from monai.transforms import \\\n",
    "    Transform, Compose, AddChannel, ScaleIntensity, ToTensor, RandRotate, RandFlip, RandZoom\n",
    "from monai.networks.nets import Generator, Discriminator\n",
    "from monai.networks import normal_init\n",
    "from monai.utils import progress_bar\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "mednist_url = 'https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz?dl=1'\n",
    "\n",
    "disc_train_interval = 1\n",
    "disc_train_steps = 5\n",
    "batch_size = 300\n",
    "latent_size = 64\n",
    "num_epochs = 50\n",
    "real_label = 1\n",
    "gen_label = 0\n",
    "learning_rate = 2e-4\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method for loading the data from the remote source differs here to demonstrate how to download and read from a tar file without using the filesystem, and because we only want the images of hand X-rays. This isn't a classification example so the category data isn't needed, so we'll download the tarball, open it using the standard library, and recall all of the file names for hands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_file = urlopen(mednist_url)\n",
    "dat=BytesIO(remote_file.read())\n",
    "\n",
    "tar=tarfile.open('MedNIST.tar.gz', fileobj=dat)\n",
    "hands = [n for n in tar.getnames() if 'Hand' in n and '.jpeg' in n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the actual image data from the tarfile, we define a transform type to do this using Matplotlib. This is used with other transforms for preparing the data followed by randomized augmentation transforms. The `CacheDataset` class is used here to cache all of the prepared images from the tarball, so we will have in memory all of the prepared images ready to be augmented with randomized rotation, flip, and zoom operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadTarJpeg(Transform):\n",
    "    def __call__(self, data):\n",
    "        return plt.imread(tar.extractfile(data))\n",
    "    \n",
    "\n",
    "train_transforms = Compose([\n",
    "    LoadTarJpeg(),\n",
    "    AddChannel(),\n",
    "    ScaleIntensity(),\n",
    "    RandRotate(range_x=15, prob=0.5, keep_size=True),\n",
    "    RandFlip(spatial_axis=0, prob=0.5),\n",
    "    RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "train_ds = monai.data.CacheDataset(hands, train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define out generator and discriminator networks. The parameters are carefully chosen to suit the image size of `(1, 64, 64)` as loaded from the tar file. Input images to the discriminator are downsampled four times to produce very small images which are flattened and passed as input to a fully-connected layer. The input latent vectors to the generator are passed through a fully-connected layer to produce an output of shape `(64, 8, 8)`, this is then upsampled three times to produce a final output which is the same shape as the real images. The networks are initialized with a normalization scheme to improve results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_net=Discriminator(\n",
    "    in_shape=(1, 64, 64),\n",
    "    channels=(8, 16, 32, 64, 1),\n",
    "    strides=(2,  2,  2,  2, 1), \n",
    "    num_res_units=1,\n",
    "    kernel_size=5\n",
    ").to(device)\n",
    "\n",
    "\n",
    "gen_net = Generator(\n",
    "    latent_shape=latent_size,\n",
    "    start_shape=(64, 8, 8),\n",
    "    channels=[32, 16, 8, 1],\n",
    "    strides=[2, 2, 2, 1]\n",
    ")\n",
    "\n",
    "# initialize both networks \n",
    "disc_net.apply(normal_init)\n",
    "gen_net.apply(normal_init)\n",
    "\n",
    "# input images are scaled to [0,1] so enforce the same of generated outputs \n",
    "gen_net.conv.add_module('activation', torch.nn.Sigmoid()) \n",
    "gen_net = gen_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the loss functions to use with helper functions to wrap the loss calculation process for the generator and the discriminator. We also define our optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_loss = torch.nn.BCELoss()\n",
    "gen_loss = torch.nn.BCELoss()\n",
    "\n",
    "disc_opt = torch.optim.Adam(disc_net.parameters(), learning_rate, betas=betas)\n",
    "gen_opt = torch.optim.Adam(gen_net.parameters(), learning_rate, betas=betas)\n",
    " \n",
    "\n",
    "def discriminator_loss(gen_images, real_images):\n",
    "    \"\"\"\n",
    "    The discriminator loss if calculated by comparing its\n",
    "    prediction for real and generated images.\n",
    "\n",
    "    \"\"\"\n",
    "    real = real_images.new_full((real_images.shape[0], 1), real_label)\n",
    "    gen = gen_images.new_full((gen_images.shape[0], 1), gen_label)\n",
    "\n",
    "    realloss = disc_loss(disc_net(real_images), real)\n",
    "    genloss = disc_loss(disc_net(gen_images.detach()), gen)\n",
    "\n",
    "    return (realloss + genloss) / 2\n",
    "\n",
    "\n",
    "def generator_loss(input):\n",
    "    \"\"\"\n",
    "    The generator loss is calculated by determining how well\n",
    "    the discriminator was fooled by the generated images.\n",
    "\n",
    "    \"\"\"\n",
    "    output = disc_net(input)\n",
    "    cats = output.new_full(output.shape, real_label)\n",
    "    return gen_loss(output, cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train by iterating over the dataset for a number of epochs. At certain after the generator training stage for each batch, the discriminator is trained for a number of steps on the same real and generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = [(0, 0)]\n",
    "gen_step_loss = []\n",
    "disc_step_loss = []\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gen_net.train()\n",
    "    disc_net.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        progress_bar(i, len(train_loader), f\"epoch {epoch + 1}, avg loss: {epoch_loss_values[-1][1]:.4f}\")\n",
    "        real_images = batch_data.to(device)\n",
    "        latent = torch.randn(real_images.shape[0], latent_size).to(device)\n",
    "        \n",
    "        gen_opt.zero_grad()\n",
    "        gen_images = gen_net(latent)\n",
    "        loss = generator_loss(gen_images)\n",
    "        loss.backward()\n",
    "        gen_opt.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        gen_step_loss.append((step,loss.item()))\n",
    "        \n",
    "        if step % disc_train_interval == 0:\n",
    "            disc_total_loss = 0\n",
    "            \n",
    "            for _ in range(disc_train_steps):\n",
    "                disc_opt.zero_grad()\n",
    "                dloss = discriminator_loss(gen_images, real_images)\n",
    "                dloss.backward()\n",
    "                disc_opt.step()\n",
    "                disc_total_loss += dloss.item()\n",
    "                \n",
    "            disc_step_loss.append((step, disc_total_loss / disc_train_steps))\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append((step, epoch_loss))\n",
    "    \n",
    "    clear_output(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separate loss values for the generator and discriminator can be graphed together. These should reach an equilibrium as the generator's ability to fool the discriminator balances with that networks ability to discriminate accurately between real and fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.semilogy(*zip(*gen_step_loss), label='Generator Loss')\n",
    "plt.semilogy(*zip(*disc_step_loss), label='Discriminator Loss')\n",
    "plt.grid(True, 'both', 'both')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we show a few randomly generated images. Hopefully most images will have four fingers and a thumb as expected (assuming polydactyl examples were not present in large numbers in the dataset). This demonstrative notebook doesn't train the networks for long, training beyond the default 50 epochs should improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10\n",
    "test_latent = torch.randn(test_size, latent_size).to(device)\n",
    "\n",
    "test_images = gen_net(test_latent)\n",
    "\n",
    "fig, axs = plt.subplots(1, test_size, figsize=(20, 4))\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(test_images[i, 0].cpu().data.numpy(), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}