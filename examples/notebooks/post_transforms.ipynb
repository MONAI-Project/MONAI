{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Transforms with spleen segmentation task\n",
    "MONAI provides post-processing transforms for handling the model outputs. Currently, the transforms include:\n",
    "- `Activations`: Adding activation layer (Sigmoid, Softmax, etc.).\n",
    "- `AsDiscrete`: Converting to discrete values (Argmax, One-Hot, Threshold value, etc).\n",
    "- `SplitChannel`: Splitting multi-channel data into multiple single channels.\n",
    "- `KeepLargestConnectedComponent`: Extracting contour of segmentation result, which can be used to map to original image and evaluate the model.\n",
    "- `LabelToContour`: Removing segmentation noise based on Connected Component Analysis.\n",
    "\n",
    "MONAI supports multiple transform chains to apply different pre-transforms or post-transforms on the same data and concatenate the results, it provides `CopyItems` transform to make copies of specified items in the data dictionary and `ConcatItems` transform to combine specified items on the expected dimension, and also provides `DeleteItems` transform to delete unnecessary items to save memory.  \n",
    "A typical usage is to scale and concatenate 3 different intensity ranges of an input image:\n",
    "<p>\n",
    "<img src=\"./images/multi_transform_chains.png\" width=\"70%\" alt='multi_transform_chains'>\n",
    "</p>\n",
    "\n",
    "This tutorial shows several of above post transforms based on the model output of spleen segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "from monai.transforms import \\\n",
    "    Compose, LoadNiftid, AddChanneld, ScaleIntensityRanged, CropForegroundd, \\\n",
    "    RandCropByPosNegLabeld, RandAffined, Spacingd, Orientationd, ToTensord, \\\n",
    "    AsDiscrete, KeepLargestConnectedComponent, LabelToContour\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.networks.layers import Norm\n",
    "from monai.utils import set_determinism\n",
    "from monai.metrics import compute_meandice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set MSD Spleen dataset path\n",
    "The Spleen dataset can be downloaded from http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/workspace/data/medical/Task09_Spleen'\n",
    "train_images = sorted(glob.glob(os.path.join(data_root, 'imagesTr', '*.nii.gz')))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_root, 'labelsTr', '*.nii.gz')))\n",
    "data_dicts = [{'image': image_name, 'label': label_name}\n",
    "              for image_name, label_name in zip(train_images, train_labels)]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    LoadNiftid(keys=['image', 'label']),\n",
    "    AddChanneld(keys=['image', 'label']),\n",
    "    Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), mode=('bilinear', 'nearest')),\n",
    "    Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "    ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "    CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "    # randomly crop out patch samples from big image based on pos / neg ratio\n",
    "    # the image centers of negative samples must be in valid image area\n",
    "    RandCropByPosNegLabeld(keys=['image', 'label'], label_key='label', spatial_size=(96, 96, 96), pos=1,\n",
    "                           neg=1, num_samples=4, image_key='image', image_threshold=0),\n",
    "    ToTensord(keys=['image', 'label'])\n",
    "])\n",
    "val_transforms = Compose([\n",
    "    LoadNiftid(keys=['image', 'label']),\n",
    "    AddChanneld(keys=['image', 'label']),\n",
    "    Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), mode=('bilinear', 'nearest')),\n",
    "    Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "    ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "    CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "    ToTensord(keys=['image', 'label'])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CacheDataset and DataLoader for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = monai.data.CacheDataset(\n",
    "    data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4\n",
    ")\n",
    "# train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "# to generate 2 x 4 images for network training\n",
    "train_loader = monai.data.DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = monai.data.CacheDataset(\n",
    "    data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4\n",
    ")\n",
    "# val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = monai.data.DataLoader(val_ds, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device('cuda:0')\n",
    "model = monai.networks.nets.UNet(dimensions=3, in_channels=1, out_channels=2, channels=(16, 32, 64, 128, 256),\n",
    "                                 strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH).to(device)\n",
    "loss_function = monai.losses.DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_num = 190\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "metric_values = list()\n",
    "for epoch in range(epoch_num):\n",
    "    print('-' * 10)\n",
    "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data['image'].to(device), batch_data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "    # validation progress\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            metric_sum = 0.\n",
    "            metric_count = 0\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = val_data['image'].to(device), val_data['label'].to(device)\n",
    "                roi_size = (160, 160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                value = compute_meandice(y_pred=val_outputs, y=val_labels, include_background=False,\n",
    "                                         to_onehot_y=True, mutually_exclusive=True)\n",
    "                metric_count += len(value)\n",
    "                metric_sum += value.sum().item()\n",
    "            metric = metric_sum / metric_count\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), 'best_metric_model.pth')\n",
    "                print('saved new best metric model')\n",
    "            print(f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                  f\"\\nbest mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute post transforms on validation dataset\n",
    "Here we test `AsDiscrete`, `KeepLargestConnectedComponent` and `LabelToContour`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_metric_model.pth'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(val_loader):\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_data = val_data['image'].to(device)\n",
    "        val_output = sliding_window_inference(val_data, roi_size, sw_batch_size, model)\n",
    "        # plot the slice [:, :, 80]\n",
    "        plt.figure('check', (20, 4))\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.title(f\"image {str(i)}\")\n",
    "        plt.imshow(val_data.detach().cpu()[0, 0, :, :, 80], cmap='gray')\n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.title(f\"argmax {str(i)}\")\n",
    "        argmax = AsDiscrete(argmax=True)(val_output)\n",
    "        plt.imshow(argmax.detach().cpu()[0, 0, :, :, 80])\n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.title(f\"largest {str(i)}\")\n",
    "        largest = KeepLargestConnectedComponent(applied_labels=[1])(argmax)\n",
    "        plt.imshow(largest.detach().cpu()[0, 0, :, :, 80])\n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.title(f\"contour {str(i)}\")\n",
    "        contour = LabelToContour()(largest)\n",
    "        plt.imshow(contour.detach().cpu()[0, 0, :, :, 80])\n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.title(f\"map image {str(i)}\")\n",
    "        map_image = contour + val_data\n",
    "        plt.imshow(map_image.detach().cpu()[0, 0, :, :, 80], cmap='gray')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}