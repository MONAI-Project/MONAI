{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models ensemble to achieve better test metrics\n",
    "Models ensemble is a popular strategy in machine learning and deep learning areas to achieve more accurate and more stable outputs.  \n",
    "A typical practice is:\n",
    "- Split all the training dataset into K folds.\n",
    "- Train K models with every K-1 folds data.\n",
    "- Execute inference on the test data with all the K models.\n",
    "- Compute the average values with weights or vote the most common value as the final result.\n",
    "<p>\n",
    "<img src=\"./images/models_ensemble.png\" width=\"80%\" alt='models_ensemble'>\n",
    "</p>\n",
    "\n",
    "MONAI provides `EnsembleEvaluator` and `MeanEnsemble`, `VoteEnsemble` post transforms.  \n",
    "This tutorial shows how to leverage ensemble modules in MONAI to set up ensemble program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import shutil\n",
    "from glob import glob\n",
    "import logging\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import monai\n",
    "from monai.transforms import Compose, LoadNiftid, AsChannelFirstd, ScaleIntensityd, \\\n",
    "    RandCropByPosNegLabeld, RandRotate90d, ToTensord, Activationsd, AsDiscreted, \\\n",
    "    MeanEnsembled, VoteEnsembled\n",
    "from monai.handlers import StatsHandler, MeanDice\n",
    "from monai.data import create_test_image_3d\n",
    "from monai.engines import SupervisedTrainer, SupervisedEvaluator, EnsembleEvaluator\n",
    "from monai.inferers import SimpleInferer, SlidingWindowInferer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random (image, label) pairs\n",
    "Generate 60 pairs for the task, 50 for training and 10 for test.  \n",
    "And then split the 50 pairs into 5 folds to train 5 separate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(60):\n",
    "    im, seg = create_test_image_3d(128, 128, 128, num_seg_classes=1, channel_dim=-1)\n",
    "    n = nib.Nifti1Image(im, np.eye(4))\n",
    "    nib.save(n, os.path.join(\"./runs\", f\"img{i:d}.nii.gz\"))\n",
    "    n = nib.Nifti1Image(seg, np.eye(4))\n",
    "    nib.save(n, os.path.join(\"./runs\", f\"seg{i:d}.nii.gz\"))\n",
    "\n",
    "images = sorted(glob(os.path.join(\"./runs\", \"img*.nii.gz\")))\n",
    "segs = sorted(glob(os.path.join(\"./runs\", \"seg*.nii.gz\")))\n",
    "\n",
    "train_files = list()\n",
    "val_files = list()\n",
    "for i in range(5):\n",
    "    train_files[i] = [{\"image\": img, \"label\": seg}\n",
    "                      for img, seg in\n",
    "                      zip(images[: (10 * i)] + images[(10 * (i + 1)) : 50],\n",
    "                          segs[: (10 * i)] + segs[(10 * (i + 1)) : 50])\n",
    "                     ]\n",
    "    val_files[i] = [{\"image\": img, \"label\": seg}\n",
    "                    for img, seg in\n",
    "                    zip(images[(10 * i) : (10 * (i + 1))], segs[(10 * i) : (10 * (i + 1))])\n",
    "                   ]\n",
    "\n",
    "test_files = [{\"image\": img, \"label\": seg} for img, seg in zip(images[50: 60], segs[50 : 60])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadNiftid(keys=[\"image\", \"label\"]),\n",
    "        AsChannelFirstd(keys=[\"image\", \"label\"], channel_dim=-1),\n",
    "        ScaleIntensityd(keys=[\"image\", \"label\"]),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"], label_key=\"label\", spatial_size=[96, 96, 96], pos=1, neg=1, num_samples=4\n",
    "        ),\n",
    "        RandRotate90d(keys=[\"image\", \"label\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "        ToTensord(keys=[\"image\", \"label\"])\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadNiftid(keys=[\"image\", \"label\"]),\n",
    "        AsChannelFirstd(keys=[\"image\", \"label\"], channel_dim=-1),\n",
    "        ScaleIntensityd(keys=[\"image\", \"label\"]),\n",
    "        ToTensord(keys=[\"image\", \"label\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CacheDatasets and DataLoaders for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 Load and cache transformed data:  [==============================]\n",
      "9/9 Load and cache transformed data:  [==============================]\n"
     ]
    }
   ],
   "source": [
    "train_dss = [monai.data.CacheDataset(data=train_files[i], transform=train_transforms) for i in range(5)]\n",
    "train_loaders = [monai.data.DataLoader(train_dss[i], batch_size=2, shuffle=True, num_workers=4) for i in range(5)]\n",
    "\n",
    "val_dss = [monai.data.CacheDataset(data=val_files, transform=val_transforms) for i in range(5)]\n",
    "val_loaders = [monai.data.DataLoader(val_ds[i], batch_size=1, num_workers=4) for i in range(5)]\n",
    "\n",
    "test_ds = monai.data.CacheDataset(data=test_files, transform=val_transforms)\n",
    "test_loader = monai.data.DataLoader(test_ds, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    net = monai.networks.nets.UNet(dimensions=3, in_channels=1, out_channels=1, channels=(16, 32, 64, 128, 256),\n",
    "                                   strides=(2, 2, 2, 2), num_res_units=2).to(device)\n",
    "    loss = monai.losses.DiceLoss(sigmoid=True)\n",
    "    opt = torch.optim.Adam(net.parameters(), 1e-3)\n",
    "\n",
    "    val_post_transforms = Compose(\n",
    "        [\n",
    "            Activationsd(keys=\"pred\", sigmoid=True),\n",
    "            AsDiscreted(keys=\"pred\", threshold_values=True)\n",
    "        ]\n",
    "    )\n",
    "    val_handlers = [StatsHandler(output_transform=lambda x: None)]\n",
    "\n",
    "    evaluator = SupervisedEvaluator(\n",
    "        device=device,\n",
    "        val_data_loader=val_loaders[index],\n",
    "        network=net,\n",
    "        inferer=SlidingWindowInferer(roi_size=(96, 96, 96), sw_batch_size=4, overlap=0.5),\n",
    "        post_transform=val_post_transforms,\n",
    "        key_val_metric={\n",
    "            \"val_mean_dice\": MeanDice(include_background=True, output_transform=lambda x: (x[\"pred\"], x[\"label\"]))\n",
    "        },\n",
    "        val_handlers=val_handlers\n",
    "    )\n",
    "    train_handlers = [\n",
    "        ValidationHandler(validator=evaluator, interval=2, epoch_level=True),\n",
    "        StatsHandler(tag_name=\"train_loss\", output_transform=lambda x: x[\"loss\"])\n",
    "    ]\n",
    "\n",
    "    trainer = SupervisedTrainer(\n",
    "        device=device,\n",
    "        max_epochs=2,\n",
    "        train_data_loader=train_loader[index],\n",
    "        network=net,\n",
    "        optimizer=opt,\n",
    "        loss_function=loss,\n",
    "        inferer=SimpleInferer(),\n",
    "        amp=False,\n",
    "        train_handlers=train_handlers\n",
    "    )\n",
    "    trainer.run()\n",
    "return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute 5 training processes and get 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [train(i) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separately evaluate the 5 models on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    test_post_transforms = Compose(\n",
    "        [\n",
    "            Activationsd(keys=\"pred\", sigmoid=True),\n",
    "            AsDiscreted(keys=\"pred\", threshold_values=True)\n",
    "        ]\n",
    "    )\n",
    "    test_handlers = [StatsHandler(output_transform=lambda x: None)]\n",
    "\n",
    "    evaluator = SupervisedEvaluator(\n",
    "        device=device,\n",
    "        val_data_loader=test_loader,\n",
    "        network=model,\n",
    "        inferer=SlidingWindowInferer(roi_size=(96, 96, 96), sw_batch_size=4, overlap=0.5),\n",
    "        post_transform=test_post_transforms,\n",
    "        key_val_metric={\n",
    "            \"test_mean_dice\": MeanDice(include_background=True, output_transform=lambda x: (x[\"pred\"], x[\"label\"]))\n",
    "        },\n",
    "        val_handlers=test_handlers\n",
    "    )\n",
    "    evaluator.run()\n",
    "\n",
    "for model in models:\n",
    "    evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the ensemble result with `MeanEnsemble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_evaluate(post_transforms, models):\n",
    "    test_handlers = [StatsHandler(output_transform=lambda x: None)]\n",
    "\n",
    "    evaluator = EnsembleEvaluator(\n",
    "        device=device,\n",
    "        val_data_loader=test_loader,\n",
    "        pred_keys=[\"pred0\", \"pred1\", \"pred2\", \"pred3\", \"pred4\"],\n",
    "        networks=models,\n",
    "        inferer=SlidingWindowInferer(roi_size=(96, 96, 96), sw_batch_size=4, overlap=0.5),\n",
    "        post_transform=post_transforms,\n",
    "        key_val_metric={\n",
    "            \"test_mean_dice\": MeanDice(include_background=True, output_transform=lambda x: (x[\"pred\"], x[\"label\"]))\n",
    "        },\n",
    "        val_handlers=test_handlers\n",
    "    )\n",
    "    evaluator.run()\n",
    "\n",
    "mean_post_transforms = Compose(\n",
    "    [\n",
    "        MeanEnsemble(\n",
    "            keys=[\"pred0\", \"pred1\", \"pred2\", \"pred3\", \"pred4\"],\n",
    "            output_key=\"pred\",\n",
    "            weights=[1 / 1, 1 / 1, 1 / 1, 1 / , 1 / 1]\n",
    "        ),\n",
    "        Activationsd(keys=\"pred\", sigmoid=True),\n",
    "        AsDiscreted(keys=\"pred\", threshold_values=True)\n",
    "    ]\n",
    ")\n",
    "ensemble_evaluate(mean_post_transforms, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the ensemble result with `VoteEnsemble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_post_transforms = Compose(\n",
    "    [\n",
    "        Activationsd(keys=[\"pred0\", \"pred1\", \"pred2\", \"pred3\", \"pred4\"], sigmoid=True),\n",
    "        AsDiscreted(keys=[\"pred0\", \"pred1\", \"pred2\", \"pred3\", \"pred4\"], threshold_values=True),\n",
    "        VoteEnsemble(\n",
    "            keys=[\"pred0\", \"pred1\", \"pred2\", \"pred3\", \"pred4\"],\n",
    "            output_key=\"pred\",\n",
    "            weights=[1 / 1, 1 / 1, 1 / 1, 1 / , 1 / 1]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "ensemble_evaluate(vote_post_transforms, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
