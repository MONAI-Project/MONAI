{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PersistentDataset, CacheDataset, and simple Dataset Tutorial and Speed Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to accelerate PyTorch medical DL program based on\n",
    "how data is loaded and preprocessed using different MONAI `Dataset` managers.\n",
    "\n",
    "`Dataset` provides the simplest model of data loading.  Each time a dataset is needed, it is reloaded from the original datasources, and processed through the all non-random and random transforms to generate analyzable tensors. This mechanism has the smallest memory footprint, and the smallest temporary disk footprint.\n",
    "\n",
    "`CacheDataset` provides a mechanism to pre-load all original data and apply non-random transforms into analyzable tensors loaded in memory prior to starting analysis.  The `CacheDataset` requires all tensor representations of data requested to be loaded into memory at once. The subset of random transforms are applied to the cached components before use. This is the highest performance dataset if all data fits in core memory.\n",
    "\n",
    "`PersistentDataset` processes original data sources through the non-random transforms on first use, and stores these intermediate tensor values to an on-disk persistence representation.  The intermediate processed tensors are loaded from disk on each use for processing by the random-transforms for each analysis request.  The `PersistentDataset` has a similar memory footprint to the simple `Dataset`, with performance characterisics close to the `CacheDataset` at the expense of disk storage.  Additially, the cost of first time processing of data is distributed across each first use.\n",
    "\n",
    "It's modified from the Spleen 3D segmentation tutorial notebook.\n",
    "\n",
    "The Spleen dataset can be downloaded from http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data import Dataset, PersistentDataset, CacheDataset, list_data_collate\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import compute_meandice\n",
    "from monai.networks.layers import Norm\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import \\\n",
    "    Compose, LoadNiftid, AddChanneld, ScaleIntensityRanged, CropForegroundd, \\\n",
    "    RandCropByPosNegLabeld, Spacingd, Orientationd, ToTensord\n",
    "from monai.utils import set_determinism\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_process(train_ds, val_ds):\n",
    "    # use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "    # to generate 2 x 4 images for network training\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, collate_fn=list_data_collate)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)\n",
    "    device = torch.device('cuda:0')\n",
    "    model = UNet(dimensions=3, in_channels=1, out_channels=2, channels=(16, 32, 64, 128, 256),\n",
    "                 strides=(2, 2, 2, 2), num_res_units=2, norm=Norm.BATCH).to(device)\n",
    "    loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "    epoch_num = 600\n",
    "    val_interval = 1  # do validation for every epoch\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "    epoch_times = list()\n",
    "    total_start = time.time()\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_start = time.time()\n",
    "        print('-' * 10)\n",
    "        print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step_start = time.time()\n",
    "            step += 1\n",
    "            inputs, labels = batch_data['image'].to(device), batch_data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            print(f\"{step}/{len(train_ds) // train_loader.batch_size}, train_loss: {loss.item():.4f}\"\n",
    "                  f\" step time: {(time.time() - step_start):.4f}\")\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                metric_sum = 0.\n",
    "                metric_count = 0\n",
    "                for val_data in val_loader:\n",
    "                    val_inputs, val_labels = val_data['image'].to(device), val_data['label'].to(device)\n",
    "                    roi_size = (160, 160, 160)\n",
    "                    sw_batch_size = 4\n",
    "                    val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                    value = compute_meandice(y_pred=val_outputs, y=val_labels, include_background=False,\n",
    "                                             to_onehot_y=True, mutually_exclusive=True)\n",
    "                    metric_count += len(value)\n",
    "                    metric_sum += value.sum().item()\n",
    "                metric = metric_sum / metric_count\n",
    "                metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), 'best_metric_model.pth')\n",
    "                    print('saved new best metric model')\n",
    "                print(f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                      f\" best mean dice: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "        print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "        epoch_times.append(time.time() - epoch_start)\n",
    "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\"\n",
    "          f\" total time: {(time.time() - total_start):.4f}\")\n",
    "    return epoch_num, time.time() - total_start, epoch_loss_values, metric_values, epoch_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of speed testing\n",
    "\n",
    "The `PersistenceDataset`, `CacheDataset`, and `Dataset` are compared for speed for running 600 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the MSD Spleen dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/workspace/data/medical/Task09_Spleen'\n",
    "train_images = sorted(glob.glob(os.path.join(data_root, 'imagesTr', '*.nii.gz')))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_root, 'labelsTr', '*.nii.gz')))\n",
    "data_dicts = [{'image': image_name, 'label': label_name}\n",
    "              for image_name, label_name in zip(train_images, train_labels)]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation\n",
    "  \n",
    "Deterministic transforms during training:  \n",
    "- LoadNiftid  \n",
    "- AddChanneld  \n",
    "- Spacingd  \n",
    "- Orientationd  \n",
    "- ScaleIntensityRanged  \n",
    "  \n",
    "Non-deterministic transforms:  \n",
    "- RandCropByPosNegLabeld  \n",
    "- ToTensord\n",
    "\n",
    "All the validation transforms are deterministic.  \n",
    "The results of all the deterministic transforms will be cached to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations():\n",
    "    train_transforms = Compose([\n",
    "        LoadNiftid(keys=['image', 'label']),\n",
    "        AddChanneld(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), mode=('bilinear', 'nearest')),\n",
    "        Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "        ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        # randomly crop out patch samples from big image based on pos / neg ratio\n",
    "        # the image centers of negative samples must be in valid image area\n",
    "        RandCropByPosNegLabeld(keys=['image', 'label'], label_key='label', spatial_size=(96, 96, 96), pos=1,\n",
    "                               neg=1, num_samples=4, image_key='image', image_threshold=0),\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ])\n",
    "\n",
    "    # NOTE: No random cropping in the validation data, we will evaluate the entire image using a sliding window.\n",
    "    val_transforms = Compose([\n",
    "        LoadNiftid(keys=['image', 'label']),\n",
    "        AddChanneld(keys=['image', 'label']),\n",
    "        Spacingd(keys=['image', 'label'], pixdim=(1.5, 1.5, 2.), mode=('bilinear', 'nearest')),\n",
    "        Orientationd(keys=['image', 'label'], axcodes='RAS'),\n",
    "        ScaleIntensityRanged(keys=['image'], a_min=-57, a_max=164, b_min=0.0, b_max=1.0, clip=True),\n",
    "        CropForegroundd(keys=['image', 'label'], source_key='image'),\n",
    "        ToTensord(keys=['image', 'label'])\n",
    "    ])\n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and regular `Dataset`\n",
    "\n",
    "Load each original dataset and transform each time it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "train_trans, val_trans = transformations()\n",
    "train_ds = Dataset(data=train_files, transform=train_trans)\n",
    "val_ds = Dataset(data=val_files, transform=val_trans)\n",
    "\n",
    "epoch_num, total_time, epoch_loss_values, metric_values, epoch_times = train_process(train_ds, val_ds)\n",
    "print(f\"total training time of {epoch_num} epochs with regular Dataset: {total_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and `PersistentDataset`\n",
    "\n",
    "Use persistent storage of non-random transformed training and validation data computed once and stored in persistently across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_cache: Path = Path(\"./persistent_cache\")\n",
    "persistent_cache.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "set_determinism(seed=0)\n",
    "train_trans, val_trans = transformations()\n",
    "train_persitence_ds = PersistentDataset(data=train_files, transform=train_trans, cache_dir=persistent_cache)\n",
    "val_persitence_ds = PersistentDataset(data=val_files, transform=val_trans, cache_dir=persistent_cache)\n",
    "\n",
    "persistence_epoch_num, persistence_total_time, persistence_epoch_loss_values, \\\n",
    "    persistence_metric_values, persistence_epoch_times = \\\n",
    "    train_process(train_persitence_ds, val_persitence_ds)\n",
    "print(f\"total training time of {persistence_epoch_num}\"\n",
    "      f\" epochs with persistent storage Dataset: {persistence_total_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable deterministic training and `CacheDataset`\n",
    "\n",
    "Precompute all non-random transforms of original data and store in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)\n",
    "train_trans, val_trans = transformations()\n",
    "cache_init_start = time.time()\n",
    "cache_train_ds = CacheDataset(data=train_files, transform=train_trans, cache_rate=1.0, num_workers=4)\n",
    "cache_val_ds = CacheDataset(data=val_files, transform=val_trans, cache_rate=1.0, num_workers=4)\n",
    "cache_init_time = time.time() - cache_init_start\n",
    "\n",
    "cache_epoch_num, cache_total_time, cache_epoch_loss_values, cache_metric_values, cache_epoch_times = \\\n",
    "    train_process(cache_train_ds, cache_val_ds)\n",
    "print(f\"total training time of {cache_epoch_num} epochs with CacheDataset: {cache_total_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('train', (12, 18))\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.title('Regular Epoch Average Loss')\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='red')\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.title('Regular Val Mean Dice')\n",
    "x = [i + 1 for i in range(len(metric_values))]\n",
    "y = cache_metric_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='red')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.title('PersistentDataset Epoch Average Loss')\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = persistence_epoch_loss_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='blue')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.title('PersistentDataset Val Mean Dice')\n",
    "x = [i + 1 for i in range(len(metric_values))]\n",
    "y = persistence_metric_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='blue')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.title('Cache Epoch Average Loss')\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = cache_epoch_loss_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='green')\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.title('Cache Val Mean Dice')\n",
    "x = [i + 1 for i in range(len(metric_values))]\n",
    "y = cache_metric_values\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.plot(x, y, color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total time and every epoch time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('train', (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Total Train Time(600 epochs)')\n",
    "plt.bar('regular', total_time, 1, label='Regular Dataset', color='red')\n",
    "plt.bar('persistent', persistence_total_time, 1, label='Persistent Dataset', color='blue')\n",
    "plt.bar('cache', cache_init_time + cache_total_time, 1, label='Cache Dataset', color='green')\n",
    "plt.bar('cache', cache_init_time, 1, label='Cache Init', color='orange')\n",
    "plt.ylabel('secs')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Epoch Time')\n",
    "x = [i + 1 for i in range(len(epoch_times))]\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('secs')\n",
    "plt.plot(x, epoch_times, label='Regular Dataset', color='red')\n",
    "plt.plot(x, persistence_epoch_times, label='Persistent Dataset', color='blue')\n",
    "plt.plot(x, cache_epoch_times, label='Cache Dataset', color='green')\n",
    "plt.grid(alpha=0.4, linestyle=':')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}